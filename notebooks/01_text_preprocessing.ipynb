{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd75564f",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "431d2a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install -q nltk\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596aa669",
   "metadata": {},
   "source": [
    "## **Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a7ffa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = '''I am Spy D. Veloper. I am a developer by essence. I resemble Miguel O'Hara from Spider-Man: Across the Spiderverse'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1b74e7",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab62a550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am Spy D. Veloper.',\n",
       " 'I am a developer by essence.',\n",
       " \"I resemble Miguel O'Hara from Spider-Man: Across the Spiderverse\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb531b4b",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d863321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'Spy',\n",
       " 'D.',\n",
       " 'Veloper',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'a',\n",
       " 'developer',\n",
       " 'by',\n",
       " 'essence',\n",
       " '.',\n",
       " 'I',\n",
       " 'resemble',\n",
       " 'Miguel',\n",
       " \"O'Hara\",\n",
       " 'from',\n",
       " 'Spider-Man',\n",
       " ':',\n",
       " 'Across',\n",
       " 'the',\n",
       " 'Spiderverse']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52270330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'Spy', 'D.', 'Veloper', '.']\n",
      "['I', 'am', 'a', 'developer', 'by', 'essence', '.']\n",
      "['I', 'resemble', 'Miguel', \"O'Hara\", 'from', 'Spider-Man', ':', 'Across', 'the', 'Spiderverse']\n"
     ]
    }
   ],
   "source": [
    "documents = sent_tokenize(corpus)\n",
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec24b619",
   "metadata": {},
   "source": [
    "### WordPunct Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf6e1749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'Spy',\n",
       " 'D',\n",
       " '.',\n",
       " 'Veloper',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'a',\n",
       " 'developer',\n",
       " 'by',\n",
       " 'essence',\n",
       " '.',\n",
       " 'I',\n",
       " 'resemble',\n",
       " 'Miguel',\n",
       " 'O',\n",
       " \"'\",\n",
       " 'Hara',\n",
       " 'from',\n",
       " 'Spider',\n",
       " '-',\n",
       " 'Man',\n",
       " ':',\n",
       " 'Across',\n",
       " 'the',\n",
       " 'Spiderverse']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffa8f95",
   "metadata": {},
   "source": [
    "### TreeBank Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "063d40cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'Spy',\n",
       " 'D.',\n",
       " 'Veloper.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'a',\n",
       " 'developer',\n",
       " 'by',\n",
       " 'essence.',\n",
       " 'I',\n",
       " 'resemble',\n",
       " 'Miguel',\n",
       " \"O'Hara\",\n",
       " 'from',\n",
       " 'Spider-Man',\n",
       " ':',\n",
       " 'Across',\n",
       " 'the',\n",
       " 'Spiderverse']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "TreebankWordTokenizer().tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1044778c",
   "metadata": {},
   "source": [
    "## **Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbcba3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['eating','eaten','eats','easily','fairly','playing','reading','doing','walking']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637b2b48",
   "metadata": {},
   "source": [
    "### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3f3b5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating --> eat\n",
      "eaten --> eaten\n",
      "eats --> eat\n",
      "easily --> easili\n",
      "fairly --> fairli\n",
      "playing --> play\n",
      "reading --> read\n",
      "doing --> do\n",
      "walking --> walk\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "for word in words:\n",
    "    print(f\"{word} --> {ps.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6963dd86",
   "metadata": {},
   "source": [
    "### RegExStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "496e270c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating --> eat\n",
      "eaten --> eaten\n",
      "eats --> eat\n",
      "easily --> easily\n",
      "fairly --> fairly\n",
      "playing --> play\n",
      "reading --> read\n",
      "doing --> do\n",
      "walking --> walk\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "rs = RegexpStemmer('ing$|s$|e$|able$')\n",
    "for word in words:\n",
    "    print(f\"{word} --> {rs.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89d556",
   "metadata": {},
   "source": [
    "### Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b8eb2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating --> eat\n",
      "eaten --> eaten\n",
      "eats --> eat\n",
      "easily --> easili\n",
      "fairly --> fair\n",
      "playing --> play\n",
      "reading --> read\n",
      "doing --> do\n",
      "walking --> walk\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "ss = SnowballStemmer('english')\n",
    "for word in words:\n",
    "    print(f\"{word} --> {ss.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd60406",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24494b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating --> eat\n",
      "eaten --> eat\n",
      "eats --> eat\n",
      "easily --> easily\n",
      "fairly --> fairly\n",
      "playing --> play\n",
      "reading --> read\n",
      "doing --> do\n",
      "walking --> walk\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "for word in words:\n",
    "    print(f\"{word} --> {wnl.lemmatize(word, pos='v')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da448d7",
   "metadata": {},
   "source": [
    "## **Stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4c90aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = '''I have a dream that one day this nation will rise up and live out the true meaning of its creed: \"We hold these truths to be self-evident, that all men are created equal.\"\n",
    "\n",
    "I have a dream that one day on the red hills of Georgia, the sons of former slaves and the sons of former slave owners will be able to sit down together at the table of brotherhood.\n",
    "\n",
    "I have a dream that one day even the state of Mississippi, a state sweltering with the heat of injustice, sweltering with the heat of oppression, will be transformed into an oasis of freedom and justice.\n",
    "\n",
    "I have a dream that my four little children will one day live in a nation where they will not be judged by the color of their skin but by the content of their character.\n",
    "\n",
    "I have a dream today!'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c4ecb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dream one day nation rise live true mean creed hold truth men creat equal\n",
      "dream one day red hill georgia son former slave son former slave owner abl sit togeth tabl brotherhood\n",
      "dream one day even state mississippi state swelter heat injustic swelter heat oppress transform oasi freedom justic\n",
      "dream four littl children one day live nation judg color skin content charact\n",
      "dream today\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "sentences = sent_tokenize(paragraph)\n",
    "for i in range(len(sentences)):\n",
    "    words = word_tokenize(sentences[i])\n",
    "    words = [ps.stem(word) for word in words if word.lower() not in stop_words and word.isalpha()]\n",
    "    sentences[i] = ' '.join(words)\n",
    "    print(sentences[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
