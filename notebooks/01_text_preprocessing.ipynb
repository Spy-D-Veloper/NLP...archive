{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a3df2e",
   "metadata": {},
   "source": [
    "# **Text Preprocesing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd77d403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2026.1.15-cp314-cp314-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\cherr\\miniconda3\\envs\\nlp\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "Downloading regex-2026.1.15-cp314-cp314-win_amd64.whl (280 kB)\n",
      "Using cached click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "\n",
      "   ---------------------------------------- 0/5 [tqdm]\n",
      "   -------- ------------------------------- 1/5 [regex]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ------------------------ --------------- 3/5 [click]\n",
      "   ------------------------ --------------- 3/5 [click]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   ---------------------------------------- 5/5 [nltk]\n",
      "\n",
      "Successfully installed click-8.3.1 joblib-1.5.3 nltk-3.9.2 regex-2026.1.15 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bf38f3",
   "metadata": {},
   "source": [
    "## **Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "010ec77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"I am Spy D. Veloper. I am a developer by essence. I resemble Miguel O'Hara from Spider-Man: Across the Spiderverse\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bbe6bc",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61f37d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8946a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am Spy D. Veloper.',\n",
       " 'I am a developer by essence.',\n",
       " \"I resemble Miguel O'Hara from Spider-Man: Across the Spiderverse\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3043b4d0",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3892bcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33093d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'Spy',\n",
       " 'D.',\n",
       " 'Veloper',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'a',\n",
       " 'developer',\n",
       " 'by',\n",
       " 'essence',\n",
       " '.',\n",
       " 'I',\n",
       " 'resemble',\n",
       " 'Miguel',\n",
       " \"O'Hara\",\n",
       " 'from',\n",
       " 'Spider-Man',\n",
       " ':',\n",
       " 'Across',\n",
       " 'the',\n",
       " 'Spiderverse']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afc7402b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'Spy', 'D.', 'Veloper', '.']\n",
      "['I', 'am', 'a', 'developer', 'by', 'essence', '.']\n",
      "['I', 'resemble', 'Miguel', \"O'Hara\", 'from', 'Spider-Man', ':', 'Across', 'the', 'Spiderverse']\n"
     ]
    }
   ],
   "source": [
    "documents = sent_tokenize(corpus)\n",
    "for sentence in documents:\n",
    "    # print(sentence)\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7378c443",
   "metadata": {},
   "source": [
    "### WordPunct Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "392a9ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'Spy',\n",
       " 'D',\n",
       " '.',\n",
       " 'Veloper',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'a',\n",
       " 'developer',\n",
       " 'by',\n",
       " 'essence',\n",
       " '.',\n",
       " 'I',\n",
       " 'resemble',\n",
       " 'Miguel',\n",
       " 'O',\n",
       " \"'\",\n",
       " 'Hara',\n",
       " 'from',\n",
       " 'Spider',\n",
       " '-',\n",
       " 'Man',\n",
       " ':',\n",
       " 'Across',\n",
       " 'the',\n",
       " 'Spiderverse']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9abb69f",
   "metadata": {},
   "source": [
    "### TreeBank Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0f48142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'Spy',\n",
       " 'D.',\n",
       " 'Veloper.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'a',\n",
       " 'developer',\n",
       " 'by',\n",
       " 'essence.',\n",
       " 'I',\n",
       " 'resemble',\n",
       " 'Miguel',\n",
       " \"O'Hara\",\n",
       " 'from',\n",
       " 'Spider-Man',\n",
       " ':',\n",
       " 'Across',\n",
       " 'the',\n",
       " 'Spiderverse']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "treebank_word_tokenize = TreebankWordTokenizer().tokenize\n",
    "treebank_word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4a442b",
   "metadata": {},
   "source": [
    "## **Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8e01b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[\"eating\",\"eaten\",\"eats\",\"easily\",\"fairly\",\"playing\",\"reading\",\"doing\",\"walking\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed65f7f4",
   "metadata": {},
   "source": [
    "### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "965999b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93b3ea2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating --> eat\n",
      "eaten --> eaten\n",
      "eats --> eat\n",
      "easily --> easili\n",
      "fairly --> fairli\n",
      "playing --> play\n",
      "reading --> read\n",
      "doing --> do\n",
      "walking --> walk\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(f\"{word} --> {ps.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c7961d",
   "metadata": {},
   "source": [
    "### RegExStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ea64a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "rs = RegexpStemmer('ing$|s$|e$|able$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a88b87f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating --> eat\n",
      "eaten --> eaten\n",
      "eats --> eat\n",
      "easily --> easily\n",
      "fairly --> fairly\n",
      "playing --> play\n",
      "reading --> read\n",
      "doing --> do\n",
      "walking --> walk\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(f\"{word} --> {rs.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f440944e",
   "metadata": {},
   "source": [
    "### Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1d87e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "ss = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c97d7c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating --> eat\n",
      "eaten --> eaten\n",
      "eats --> eat\n",
      "easily --> easili\n",
      "fairly --> fair\n",
      "playing --> play\n",
      "reading --> read\n",
      "doing --> do\n",
      "walking --> walk\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(f\"{word} --> {ss.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41176bac",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f10a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[\"eating\",\"eaten\",\"eats\",\"easily\",\"fairly\",\"playing\",\"reading\",\"doing\",\"walking\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa3686c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dea5469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating --> eat\n",
      "eaten --> eat\n",
      "eats --> eat\n",
      "easily --> easily\n",
      "fairly --> fairly\n",
      "playing --> play\n",
      "reading --> read\n",
      "doing --> do\n",
      "walking --> walk\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(f\"{word} --> {wnl.lemmatize(word, pos='v')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55366657",
   "metadata": {},
   "source": [
    "## **Stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af0f34f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"I have a dream that one day this nation will rise up and live out the true meaning of its creed: \"We hold these truths to be self-evident, that all men are created equal.\" \n",
    "\n",
    "I have a dream that one day on the red hills of Georgia, the sons of former slaves and the sons of former slave owners will be able to sit down together at the table of brotherhood.\n",
    "\n",
    "I have a dream that one day even the state of Mississippi, a state sweltering with the heat of injustice, sweltering with the heat of oppression, will be transformed into an oasis of freedom and justice.\n",
    "\n",
    "I have a dream that my four little children will one day live in a nation where they will not be judged by the color of their skin but by the content of their character.\n",
    "\n",
    "I have a dream today!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a1c4992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cherr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf909800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efbe28bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dream one day nation rise live true mean creed : `` hold truth self-evid , men creat equal . ''\n",
      "dream one day red hill georgia , son former slave son former slave owner abl sit togeth tabl brotherhood .\n",
      "dream one day even state mississippi , state swelter heat injustic , swelter heat oppress , transform oasi freedom justic .\n",
      "dream four littl children one day live nation judg color skin content charact .\n",
      "dream today !\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [ss.stem(word) for word in words if word.lower() not in stopwords.words('english')]\n",
    "    sentences[i] = ' '.join(words) \n",
    "    print(sentences[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
